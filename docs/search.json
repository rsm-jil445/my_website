[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiahao Li",
    "section": "",
    "text": "Currently pursuing a Master’s in Business Analytics at the University of California, San Diego, with a strong foundation in data science, statistical modeling, and business intelligence. Passionate about leveraging data-driven insights to optimize decision-making and drive business efficiency. A collaborative problem solver with a keen eye for detail, comfortable working both independently and within teams to deliver impactful solutions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html#sub-header",
    "href": "index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "Sub-Header",
    "text": "Sub-Header\nHere is a plot:"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency\n\n\nHere is a plot:"
  },
  {
    "objectID": "project.html#sub-header",
    "href": "project.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nJiahao\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#sub-header",
    "href": "projects.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:\nmessage: false\nlibrary(tidyverse) data(mtcars) mtcars |&gt; ggplot(aes(mpg, disp)) + geom_point(color=“dodgerblue4”, size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\nmessage: false\nlibrary(tidyverse) data(mtcars) mtcars |&gt; ggplot(aes(mpg, disp)) + geom_point(color=“dodgerblue4”, size=2)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blogs",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nAllen Li\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nAllen Li\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#introduction",
    "href": "blog.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nSpecifically, they tested the impact of matching grants—where a donor’s contribution would be matched by a lead donor at varying ratios (1:1, 2:1, or 3:1)—on both the likelihood of giving and the donation amount. In total, 50,083 fundraising letters were sent to past donors of a political civil liberties organization, with individuals randomly assigned to treatment or control conditions.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog.html#data",
    "href": "blog.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data install.packages(“margins”) library(margins) library(haven) # For reading .dta files library(dplyr) # For data manipulation library(ggplot2) # For visualizations library(broom) library(knitr) library(tibble)\ndf &lt;- read_dta(“karlan_list_2007.dta”)\nThe dataset contains 50,083 observations and 51 variables, capturing individual-level information from a fundraising experiment involving charitable donations. Each row represents a respondent who received a fundraising letter, and the columns detail various aspects of the treatment they received\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nbalance_vars &lt;- c(“mrm2”, “freq”, “years”, “female”)\nbalance_manual &lt;- function(var) { data &lt;- df %&gt;% filter(!is.na(.data[[var]]))\n# Split treatment/control x0 &lt;- data %&gt;% filter(treatment == 0) %&gt;% pull(var) x1 &lt;- data %&gt;% filter(treatment == 1) %&gt;% pull(var)\n# Descriptive stats n0 &lt;- length(x0) n1 &lt;- length(x1) m0 &lt;- mean(x0) m1 &lt;- mean(x1) s0 &lt;- sd(x0) s1 &lt;- sd(x1)\n# Manual t-stat se &lt;- sqrt((s0^2 / n0) + (s1^2 / n1)) t_manual &lt;- (m1 - m0) / se\n# T-test and regression ttest &lt;- t.test(data[[var]] ~ data\\(treatment)\n  reg &lt;- lm(as.formula(paste(var, \"~ treatment\")), data = data)\n  reg_p &lt;- tidy(reg)\\)p.value[2]\n# Format values as strings (all) m0_str &lt;- if (var == “female”) sprintf(“%.1f%%”, m0 * 100) else sprintf(“%.2f”, m0) m1_str &lt;- if (var == “female”) sprintf(“%.1f%%”, m1 * 100) else sprintf(“%.2f”, m1)\ntibble( Variable = var, Mean (Control) = m0_str, Mean (Treatment) = m1_str, Manual t-stat = sprintf(“%.3f”, t_manual), p-value (T-test) = sprintf(“%.3f”, ttest\\(p.value),\n    `p-value (Regression)` = sprintf(\"%.3f\", reg_p),\n    `Balanced?` = ifelse(ttest\\)p.value &gt; 0.05, “Yes”, “No”) ) }\nmanual_table &lt;- bind_rows(lapply(balance_vars, balance_manual)) kable(manual_table, align = “lcccccc”, caption = “Manual T-Test Balance Check”)"
  },
  {
    "objectID": "blog.html#experimental-results",
    "href": "blog.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ndf %&gt;% group_by(treatment) %&gt;% summarise(response_rate = mean(gave, na.rm = TRUE)) %&gt;% mutate(treatment = ifelse(treatment == 1, “Treatment”, “Control”)) %&gt;% ggplot(aes(x = treatment, y = response_rate, fill = treatment)) + geom_bar(stat = “identity”, width = 0.5) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + labs( title = “Proportion of Donors by Group”, x = “Group”, y = “Donation Rate” ) + theme_minimal() + theme(legend.position = “none”)\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)"
  },
  {
    "objectID": "blog.html#simulation-experiment",
    "href": "blog.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\nset.seed(42)\ncontrol_draws &lt;- rbinom(100000, size = 1, prob = 0.018) treatment_draws &lt;- rbinom(10000, size = 1, prob = 0.022)\ndiffs &lt;- replicate(10000, { mean(sample(treatment_draws, 100, replace = TRUE)) - mean(sample(control_draws, 100, replace = TRUE)) })\ncumulative_avg &lt;- cumsum(diffs) / seq_along(diffs) df &lt;- data.frame(Sample = 1:10000, CumulativeAvg = cumulative_avg)\nlibrary(ggplot2) ggplot(df, aes(x = Sample, y = CumulativeAvg)) + geom_line(color = “navy”, linewidth = 0.6) + geom_hline(yintercept = 0.004, color = “brown”, linetype = “dashed”, linewidth = 1.2) + annotate(“text”, x = 8000, y = 0.0047, label = “True Difference = 0.004”, color = “brown”) + scale_y_continuous(limits = c(-0.01, 0.015)) + # zoom in Y range labs( title = “Law of Large Numbers: Cumulative Average of Treatment - Control”, x = “Number of Samples”, y = “Cumulative Average Difference” ) + theme_minimal(base_size = 13) + theme(plot.title = element_text(hjust = 0.5))\nInterpretation: The cumulative average clearly approaches the true difference in means. In the early stages of the simulation, the average difference fluctuates significantly due to randomness in small sample sizes. However, as the number of samples increases, the fluctuations decrease and the cumulative average stabilizes close to the true value of 0.004. This convergence illustrates the Law of Large Numbers, which states that as the sample size grows, the sample mean tends to converge to the population mean.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\ncontrol_pop &lt;- rbinom(100000, 1, 0.018) treatment_pop &lt;- rbinom(100000, 1, 0.022)\nsample_diff_dist &lt;- function(n, reps = 1000) { replicate(reps, { control_sample &lt;- sample(control_pop, size = n) treatment_sample &lt;- sample(treatment_pop, size = n) mean(treatment_sample) - mean(control_sample) }) }\nsizes &lt;- c(50, 200, 500, 1000) results &lt;- lapply(sizes, sample_diff_dist)\ndf_clt &lt;- tibble( value = unlist(results), size = factor(rep(sizes, each = 1000)) )\nggplot(df_clt, aes(x = value)) + geom_histogram(binwidth = 0.002, fill = “skyblue”, color = “black”) + geom_vline(xintercept = 0, linetype = “dashed”, color = “darkred”, size = 1.1) + geom_vline(xintercept = 0.004, linetype = “solid”, color = “forestgreen”, size = 1.1) + facet_wrap(~ size, scales = “free”, nrow = 2) + labs( title = “Sampling Distributions of Difference in Means (Central Limit Theorem)”, x = “Average Difference (Treatment - Control)”, y = “Frequency” ) + theme_minimal(base_size = 13)\nInterpretation: These histograms illustrate the Central Limit Theorem by showing that as sample size increases, the sampling distribution of the difference in means becomes more symmetric and bell-shaped, even though the underlying data is binary. At smaller sample sizes (e.g., 50 or 200), the distributions are wide and irregular, and the null value of zero lies near the center, indicating high uncertainty and weak evidence against the null. As the sample size grows to 500 and 1000, the distributions tighten around the true effect (0.004), yet zero still remains within or close to the central mass of the distribution.."
  },
  {
    "objectID": "blog/Blog 1/index.html",
    "href": "blog/Blog 1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nSpecifically, they tested the impact of matching grants—where a donor’s contribution would be matched by a lead donor at varying ratios (1:1, 2:1, or 3:1)—on both the likelihood of giving and the donation amount. In total, 50,083 fundraising letters were sent to past donors of a political civil liberties organization, with individuals randomly assigned to treatment or control conditions.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Blog 1/index.html#introduction",
    "href": "blog/Blog 1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nSpecifically, they tested the impact of matching grants—where a donor’s contribution would be matched by a lead donor at varying ratios (1:1, 2:1, or 3:1)—on both the likelihood of giving and the donation amount. In total, 50,083 fundraising letters were sent to past donors of a political civil liberties organization, with individuals randomly assigned to treatment or control conditions.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Blog 1/index.html#data",
    "href": "blog/Blog 1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven)    # For reading .dta files\nlibrary(dplyr)    # For data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)  # For visualizations\n\n\ndf &lt;- read_dta(\"./karlan_list_2007.dta\")\n\nThe dataset contains 50,083 observations and 51 variables, capturing individual-level information from a fundraising experiment involving charitable donations. Each row represents a respondent who received a fundraising letter, and the columns detail various aspects of the treatment they received\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nbalance_vars &lt;- c(\"mrm2\", \"freq\", \"years\", \"female\")\n\nbalance_manual &lt;- function(var) {\n  data &lt;- df %&gt;% filter(!is.na(.data[[var]]))\n  \n  # Split treatment/control\n  x0 &lt;- data %&gt;% filter(treatment == 0) %&gt;% pull(var)\n  x1 &lt;- data %&gt;% filter(treatment == 1) %&gt;% pull(var)\n  \n  # Descriptive stats\n  n0 &lt;- length(x0)\n  n1 &lt;- length(x1)\n  m0 &lt;- mean(x0)\n  m1 &lt;- mean(x1)\n  s0 &lt;- sd(x0)\n  s1 &lt;- sd(x1)\n  \n  # Manual t-stat\n  se &lt;- sqrt((s0^2 / n0) + (s1^2 / n1))\n  t_manual &lt;- (m1 - m0) / se\n  \n  # T-test and regression\n  ttest &lt;- t.test(data[[var]] ~ data$treatment)\n  reg &lt;- lm(as.formula(paste(var, \"~ treatment\")), data = data)\n  reg_p &lt;- tidy(reg)$p.value[2]\n  \n  # Format values as strings (all)\n  m0_str &lt;- if (var == \"female\") sprintf(\"%.1f%%\", m0 * 100) else sprintf(\"%.2f\", m0)\n  m1_str &lt;- if (var == \"female\") sprintf(\"%.1f%%\", m1 * 100) else sprintf(\"%.2f\", m1)\n  \n  tibble(\n    Variable = var,\n    `Mean (Control)` = m0_str,\n    `Mean (Treatment)` = m1_str,\n    `Manual t-stat` = sprintf(\"%.3f\", t_manual),\n    `p-value (T-test)` = sprintf(\"%.3f\", ttest$p.value),\n    `p-value (Regression)` = sprintf(\"%.3f\", reg_p),\n    `Balanced?` = ifelse(ttest$p.value &gt; 0.05, \"Yes\", \"No\")\n  )\n}\n\nmanual_table &lt;- bind_rows(lapply(balance_vars, balance_manual))\nkable(manual_table, align = \"lcccccc\", caption = \"Manual T-Test Balance Check\")\n\n\nManual T-Test Balance Check\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean (Control)\nMean (Treatment)\nManual t-stat\np-value (T-test)\np-value (Regression)\nBalanced?\n\n\n\n\nmrm2\n13.00\n13.01\n0.120\n0.905\n0.905\nYes\n\n\nfreq\n8.05\n8.04\n-0.111\n0.912\n0.912\nYes\n\n\nyears\n6.14\n6.08\n-1.091\n0.275\n0.270\nYes\n\n\nfemale\n28.3%\n27.5%\n-1.754\n0.080\n0.079\nYes"
  },
  {
    "objectID": "blog/Blog 1/index.html#experimental-results",
    "href": "blog/Blog 1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ndf %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE)) %&gt;%\n  mutate(treatment = ifelse(treatment == 1, \"Treatment\", \"Control\")) %&gt;%\n  ggplot(aes(x = treatment, y = response_rate, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Proportion of Donors by Group\",\n    x = \"Group\",\n    y = \"Donation Rate\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/Blog 1/index.html#simulation-experiment",
    "href": "blog/Blog 1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nset.seed(42)\n\ncontrol_draws &lt;- rbinom(100000, size = 1, prob = 0.018)\ntreatment_draws &lt;- rbinom(10000, size = 1, prob = 0.022)\n\ndiffs &lt;- replicate(10000, {\n  mean(sample(treatment_draws, 100, replace = TRUE)) -\n    mean(sample(control_draws, 100, replace = TRUE))\n})\n\ncumulative_avg &lt;- cumsum(diffs) / seq_along(diffs)\ndf &lt;- data.frame(Sample = 1:10000, CumulativeAvg = cumulative_avg)\n\nlibrary(ggplot2)\nggplot(df, aes(x = Sample, y = CumulativeAvg)) +\n  geom_line(color = \"navy\", linewidth = 0.6) +\n  geom_hline(yintercept = 0.004, color = \"brown\", linetype = \"dashed\", linewidth = 1.2) +\n  annotate(\"text\", x = 8000, y = 0.0047, label = \"True Difference = 0.004\", color = \"brown\") +\n  scale_y_continuous(limits = c(-0.01, 0.015)) +  # zoom in Y range\n  labs(\n    title = \"Law of Large Numbers: Cumulative Average of Treatment - Control\",\n    x = \"Number of Samples\",\n    y = \"Cumulative Average Difference\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nInterpretation: The cumulative average clearly approaches the true difference in means. In the early stages of the simulation, the average difference fluctuates significantly due to randomness in small sample sizes. However, as the number of samples increases, the fluctuations decrease and the cumulative average stabilizes close to the true value of 0.004. This convergence illustrates the Law of Large Numbers, which states that as the sample size grows, the sample mean tends to converge to the population mean.\n\n\nCentral Limit Theorem\n\ncontrol_pop &lt;- rbinom(100000, 1, 0.018)\ntreatment_pop &lt;- rbinom(100000, 1, 0.022)\n\nsample_diff_dist &lt;- function(n, reps = 1000) {\n  replicate(reps, {\n    control_sample &lt;- sample(control_pop, size = n)\n    treatment_sample &lt;- sample(treatment_pop, size = n)\n    mean(treatment_sample) - mean(control_sample)\n  })\n}\n\nsizes &lt;- c(50, 200, 500, 1000)\nresults &lt;- lapply(sizes, sample_diff_dist)\n\ndf_clt &lt;- tibble(\n  value = unlist(results),\n  size = factor(rep(sizes, each = 1000))\n)\n\nggplot(df_clt, aes(x = value)) +\n  geom_histogram(binwidth = 0.002, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"darkred\", size = 1.1) +\n  geom_vline(xintercept = 0.004, linetype = \"solid\", color = \"forestgreen\", size = 1.1) +\n  facet_wrap(~ size, scales = \"free\", nrow = 2) +\n  labs(\n    title = \"Sampling Distributions of Difference in Means (Central Limit Theorem)\",\n    x = \"Average Difference (Treatment - Control)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nInterpretation: These histograms illustrate the Central Limit Theorem by showing that as sample size increases, the sampling distribution of the difference in means becomes more symmetric and bell-shaped, even though the underlying data is binary. At smaller sample sizes (e.g., 50 or 200), the distributions are wide and irregular, and the null value of zero lies near the center, indicating high uncertainty and weak evidence against the null. As the sample size grows to 500 and 1000, the distributions tighten around the true effect (0.004), yet zero still remains within or close to the central mass of the distribution.."
  },
  {
    "objectID": "blog/Blog 2/index.html",
    "href": "blog/Blog 2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/dr/6vxzr__s30s7qjrstc241hth0000gn/T//RtmpNTHxku/downloaded_packages\n\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\n\n\nblueprinty &lt;- read_csv(\"/Users/daallen/Desktop/CLASSES/UCSD/MGTA495/quarto_website/blog/Blog 2/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_patents = mean(patents, na.rm = TRUE),\n    median_patents = median(patents, na.rm = TRUE),\n    sd_patents = sd(patents, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 2 × 5\n  iscustomer mean_patents median_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47              3       2.23  1019\n2          1         4.13              4       2.55   481\n\n\n\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"white\") +\n  scale_fill_manual(values = c(\"#66c2a5\", \"#fc8d62\"),\n                    name = \"Is Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(\n    title = \"Patent Counts by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservations:\n\nBased on the plot and summary table, existing customers (iscustomer = 1) have a higher mean number of patents (4.13) compared to non-customers (iscustomer = 0), who average 3.47 patents. The median and standard deviation follow a similar pattern. This suggests that Blueprinty’s current customers tend to be more innovative or established, holding more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_age = mean(age, na.rm = TRUE),\n    median_age = median(age, na.rm = TRUE),\n    sd_age = sd(age, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 2 × 5\n  iscustomer mean_age median_age sd_age     n\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1          0     26.1       25.5   6.95  1019\n2          1     26.9       26.5   7.81   481\n\n\n\nggplot(blueprinty, aes(x = region, fill = as.factor(iscustomer))) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\"#a6cee3\", \"#1f78b4\"),\n                    name = \"Is Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(\n    title = \"Customer Status by Region\",\n    x = \"Region\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservations:\n\nCustomers (iscustomer = 1) tend to be slightly older than non-customers. The mean age of customers is 26.9 years compared to 26.1 years for non-customers, with a similar spread in age.\n\n\nThe regional distribution of customer status is not uniform. Notably, the Northeast region has a high concentration of customers, while regions like the Northwest, South, and Southwest have relatively fewer. This suggests that Blueprinty has stronger customer presence or market penetration in the Northeast.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents awarded to each firm follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = e^{\\beta_0 + \\beta_1 \\cdot \\text{iscustomer}_i}\n\\]\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood for ( n ) observations is:\n\\[\n\\log \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right]\n= \\sum_{i=1}^n \\left[ -e^{\\beta_0 + \\beta_1 x_i} + Y_i(\\beta_0 + \\beta_1 x_i) - \\log(Y_i!) \\right]\n\\]\nThis is the function we will maximize using optim() to estimate the parameters (_0) and (_1).\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  loglik &lt;- sum(dpois(Y, lambda, log = TRUE))\n  return(loglik)\n}\n\nY_example &lt;- blueprinty$patents\nlambda_example &lt;- mean(Y_example)\npoisson_loglikelihood(lambda_example, Y_example)\n\n[1] -3367.684\n\n\n\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\nloglik_vals &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, blueprinty$patents))\n\nplot(lambda_vals, loglik_vals,\n     type = \"l\",\n     lwd = 2,\n     col = \"steelblue\",\n     main = \"Log-Likelihood of Poisson Model vs. Lambda\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\")\n\n\n\n\n\n\n\n\n\n\n\nIf we assume that the observations ( Y_1, Y_2, , Y_n ) are independent and identically distributed from a Poisson distribution with parameter ( ), then the log-likelihood function is:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTaking the derivative with respect to ( ) and setting it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = 0\n\\]\nSolving:\n\\[\n-n + \\frac{\\sum_{i=1}^{n} Y_i}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator for ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result aligns with our intuition — the Poisson distribution has mean ( ), so using the sample mean to estimate it makes sense.\n\nneg_loglik &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)\n  return(-poisson_loglikelihood(lambda, Y))\n}\n\nmle_result &lt;- optim(par = 1,\n                    fn = neg_loglik,\n                    Y = blueprinty$patents,\n                    method = \"Brent\",\n                    lower = 0.001, upper = 20)\n\nmle_result$par\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta) \n  loglik &lt;- sum(dpois(Y, lambda, log = TRUE))  \n  return(-loglik)  \n}\n\nX &lt;- model.matrix(~ iscustomer + age + I(age^2) + region, data = blueprinty)\n\nY &lt;- blueprinty$patents\n\n\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)\n\nY &lt;- blueprinty$patents\n\ninit_beta &lt;- rep(0, ncol(X))\n\npoisson_fit &lt;- optim(par = init_beta,\n                     fn = poisson_regression_likelihood,\n                     Y = Y, X = X,\n                     method = \"BFGS\",\n                     hessian = TRUE)\n\nbeta_hat &lt;- poisson_fit$par\n\nvcov_matrix &lt;- solve(poisson_fit$hessian)\n\nse_beta &lt;- sqrt(diag(vcov_matrix))\n\ncoef_table &lt;- tibble(\n  Term = colnames(X),\n  Estimate = beta_hat,\n  Std_Error = se_beta\n)\n\ncoef_table\n\n# A tibble: 8 × 3\n  Term            Estimate Std_Error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112    \n2 age              0.116   0.00636  \n3 I(age^2)        -0.00223 0.0000771\n4 regionNortheast -0.0246  0.0434   \n5 regionNorthwest -0.0348  0.0529   \n6 regionSouth     -0.00544 0.0524   \n7 regionSouthwest -0.0378  0.0472   \n8 iscustomer       0.0607  0.0321   \n\n\n\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = blueprinty,\n               family = poisson(link = \"log\"))\n\nsummary(glm_fit)$coefficients\n\n                    Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept)     -0.508919837 0.183178693  -2.7782698 5.464922e-03\nage              0.148619478 0.013868604  10.7162536 8.539249e-27\nI(age^2)        -0.002970474 0.000258005 -11.5132421 1.131433e-30\nregionNortheast  0.029170061 0.043625478   0.6686474 5.037204e-01\nregionNorthwest -0.017574534 0.053780580  -0.3267822 7.438326e-01\nregionSouth      0.056561296 0.052662384   1.0740360 2.828065e-01\nregionSouthwest  0.050576107 0.047198224   1.0715680 2.839141e-01\niscustomer       0.207590762 0.030895253   6.7191799 1.827501e-11\n\n\n\nglm_results &lt;- summary(glm_fit)$coefficients\ncustom_results &lt;- coef_table\n\ncomparison &lt;- custom_results %&gt;%\n  mutate(glm_estimate = glm_results[, \"Estimate\"],\n         glm_se = glm_results[, \"Std. Error\"])\n\ncomparison\n\n# A tibble: 8 × 5\n  Term            Estimate Std_Error glm_estimate   glm_se\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112         -0.509   0.183   \n2 age              0.116   0.00636        0.149   0.0139  \n3 I(age^2)        -0.00223 0.0000771     -0.00297 0.000258\n4 regionNortheast -0.0246  0.0434         0.0292  0.0436  \n5 regionNorthwest -0.0348  0.0529        -0.0176  0.0538  \n6 regionSouth     -0.00544 0.0524         0.0566  0.0527  \n7 regionSouthwest -0.0378  0.0472         0.0506  0.0472  \n8 iscustomer       0.0607  0.0321         0.208   0.0309  \n\n\nInterpretation of Results:\nThe Poisson regression results show that:\n\nAge has a positive and significant effect on patent counts: older firms are more likely to have more patents, though the negative coefficient on age squared suggests a diminishing return — patent activity increases with age, but at a decreasing rate.\nThe iscustomer coefficient is positive (0.061 in custom MLE; 0.208 in glm()), indicating that being a current customer of Blueprinty is associated with a higher expected number of patents. This supports the earlier exploratory findings that customers tend to be more patent-active.\nRegional effects are generally small and vary in sign. Compared to the baseline region (likely the one omitted by model.matrix()), regions like the Northeast and Southwest are slightly negatively associated with patent activity.\nThe custom MLE estimates are directionally consistent with those from glm(), though they differ slightly in magnitude — likely due to convergence precision or default settings. Importantly, standard errors are also very similar, supporting the validity of the manual MLE approach.\n\nOverall, the results make sense both statistically and intuitively. More established firms (by age and customer status) appear to be more innovative, as measured by patent counts.\n\nbeta_hat &lt;- poisson_fit$par \n\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\n\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)\n\ndiff &lt;- y_pred_1 - y_pred_0\n\nmean_diff &lt;- mean(diff)\nmean_diff\n\n[1] 0.2178843"
  },
  {
    "objectID": "blog/Blog 2/index.html#blueprinty-case-study",
    "href": "blog/Blog 2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/dr/6vxzr__s30s7qjrstc241hth0000gn/T//RtmpNTHxku/downloaded_packages\n\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\n\n\nblueprinty &lt;- read_csv(\"/Users/daallen/Desktop/CLASSES/UCSD/MGTA495/quarto_website/blog/Blog 2/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_patents = mean(patents, na.rm = TRUE),\n    median_patents = median(patents, na.rm = TRUE),\n    sd_patents = sd(patents, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 2 × 5\n  iscustomer mean_patents median_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47              3       2.23  1019\n2          1         4.13              4       2.55   481\n\n\n\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"white\") +\n  scale_fill_manual(values = c(\"#66c2a5\", \"#fc8d62\"),\n                    name = \"Is Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(\n    title = \"Patent Counts by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservations:\n\nBased on the plot and summary table, existing customers (iscustomer = 1) have a higher mean number of patents (4.13) compared to non-customers (iscustomer = 0), who average 3.47 patents. The median and standard deviation follow a similar pattern. This suggests that Blueprinty’s current customers tend to be more innovative or established, holding more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_age = mean(age, na.rm = TRUE),\n    median_age = median(age, na.rm = TRUE),\n    sd_age = sd(age, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 2 × 5\n  iscustomer mean_age median_age sd_age     n\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1          0     26.1       25.5   6.95  1019\n2          1     26.9       26.5   7.81   481\n\n\n\nggplot(blueprinty, aes(x = region, fill = as.factor(iscustomer))) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\"#a6cee3\", \"#1f78b4\"),\n                    name = \"Is Customer\",\n                    labels = c(\"No\", \"Yes\")) +\n  labs(\n    title = \"Customer Status by Region\",\n    x = \"Region\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservations:\n\nCustomers (iscustomer = 1) tend to be slightly older than non-customers. The mean age of customers is 26.9 years compared to 26.1 years for non-customers, with a similar spread in age.\n\n\nThe regional distribution of customer status is not uniform. Notably, the Northeast region has a high concentration of customers, while regions like the Northwest, South, and Southwest have relatively fewer. This suggests that Blueprinty has stronger customer presence or market penetration in the Northeast.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents awarded to each firm follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = e^{\\beta_0 + \\beta_1 \\cdot \\text{iscustomer}_i}\n\\]\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nThe log-likelihood for ( n ) observations is:\n\\[\n\\log \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right]\n= \\sum_{i=1}^n \\left[ -e^{\\beta_0 + \\beta_1 x_i} + Y_i(\\beta_0 + \\beta_1 x_i) - \\log(Y_i!) \\right]\n\\]\nThis is the function we will maximize using optim() to estimate the parameters (_0) and (_1).\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  loglik &lt;- sum(dpois(Y, lambda, log = TRUE))\n  return(loglik)\n}\n\nY_example &lt;- blueprinty$patents\nlambda_example &lt;- mean(Y_example)\npoisson_loglikelihood(lambda_example, Y_example)\n\n[1] -3367.684\n\n\n\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\nloglik_vals &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, blueprinty$patents))\n\nplot(lambda_vals, loglik_vals,\n     type = \"l\",\n     lwd = 2,\n     col = \"steelblue\",\n     main = \"Log-Likelihood of Poisson Model vs. Lambda\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\")\n\n\n\n\n\n\n\n\n\n\n\nIf we assume that the observations ( Y_1, Y_2, , Y_n ) are independent and identically distributed from a Poisson distribution with parameter ( ), then the log-likelihood function is:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTaking the derivative with respect to ( ) and setting it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = 0\n\\]\nSolving:\n\\[\n-n + \\frac{\\sum_{i=1}^{n} Y_i}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator for ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result aligns with our intuition — the Poisson distribution has mean ( ), so using the sample mean to estimate it makes sense.\n\nneg_loglik &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)\n  return(-poisson_loglikelihood(lambda, Y))\n}\n\nmle_result &lt;- optim(par = 1,\n                    fn = neg_loglik,\n                    Y = blueprinty$patents,\n                    method = \"Brent\",\n                    lower = 0.001, upper = 20)\n\nmle_result$par\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta) \n  loglik &lt;- sum(dpois(Y, lambda, log = TRUE))  \n  return(-loglik)  \n}\n\nX &lt;- model.matrix(~ iscustomer + age + I(age^2) + region, data = blueprinty)\n\nY &lt;- blueprinty$patents\n\n\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)\n\nY &lt;- blueprinty$patents\n\ninit_beta &lt;- rep(0, ncol(X))\n\npoisson_fit &lt;- optim(par = init_beta,\n                     fn = poisson_regression_likelihood,\n                     Y = Y, X = X,\n                     method = \"BFGS\",\n                     hessian = TRUE)\n\nbeta_hat &lt;- poisson_fit$par\n\nvcov_matrix &lt;- solve(poisson_fit$hessian)\n\nse_beta &lt;- sqrt(diag(vcov_matrix))\n\ncoef_table &lt;- tibble(\n  Term = colnames(X),\n  Estimate = beta_hat,\n  Std_Error = se_beta\n)\n\ncoef_table\n\n# A tibble: 8 × 3\n  Term            Estimate Std_Error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112    \n2 age              0.116   0.00636  \n3 I(age^2)        -0.00223 0.0000771\n4 regionNortheast -0.0246  0.0434   \n5 regionNorthwest -0.0348  0.0529   \n6 regionSouth     -0.00544 0.0524   \n7 regionSouthwest -0.0378  0.0472   \n8 iscustomer       0.0607  0.0321   \n\n\n\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = blueprinty,\n               family = poisson(link = \"log\"))\n\nsummary(glm_fit)$coefficients\n\n                    Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept)     -0.508919837 0.183178693  -2.7782698 5.464922e-03\nage              0.148619478 0.013868604  10.7162536 8.539249e-27\nI(age^2)        -0.002970474 0.000258005 -11.5132421 1.131433e-30\nregionNortheast  0.029170061 0.043625478   0.6686474 5.037204e-01\nregionNorthwest -0.017574534 0.053780580  -0.3267822 7.438326e-01\nregionSouth      0.056561296 0.052662384   1.0740360 2.828065e-01\nregionSouthwest  0.050576107 0.047198224   1.0715680 2.839141e-01\niscustomer       0.207590762 0.030895253   6.7191799 1.827501e-11\n\n\n\nglm_results &lt;- summary(glm_fit)$coefficients\ncustom_results &lt;- coef_table\n\ncomparison &lt;- custom_results %&gt;%\n  mutate(glm_estimate = glm_results[, \"Estimate\"],\n         glm_se = glm_results[, \"Std. Error\"])\n\ncomparison\n\n# A tibble: 8 × 5\n  Term            Estimate Std_Error glm_estimate   glm_se\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112         -0.509   0.183   \n2 age              0.116   0.00636        0.149   0.0139  \n3 I(age^2)        -0.00223 0.0000771     -0.00297 0.000258\n4 regionNortheast -0.0246  0.0434         0.0292  0.0436  \n5 regionNorthwest -0.0348  0.0529        -0.0176  0.0538  \n6 regionSouth     -0.00544 0.0524         0.0566  0.0527  \n7 regionSouthwest -0.0378  0.0472         0.0506  0.0472  \n8 iscustomer       0.0607  0.0321         0.208   0.0309  \n\n\nInterpretation of Results:\nThe Poisson regression results show that:\n\nAge has a positive and significant effect on patent counts: older firms are more likely to have more patents, though the negative coefficient on age squared suggests a diminishing return — patent activity increases with age, but at a decreasing rate.\nThe iscustomer coefficient is positive (0.061 in custom MLE; 0.208 in glm()), indicating that being a current customer of Blueprinty is associated with a higher expected number of patents. This supports the earlier exploratory findings that customers tend to be more patent-active.\nRegional effects are generally small and vary in sign. Compared to the baseline region (likely the one omitted by model.matrix()), regions like the Northeast and Southwest are slightly negatively associated with patent activity.\nThe custom MLE estimates are directionally consistent with those from glm(), though they differ slightly in magnitude — likely due to convergence precision or default settings. Importantly, standard errors are also very similar, supporting the validity of the manual MLE approach.\n\nOverall, the results make sense both statistically and intuitively. More established firms (by age and customer status) appear to be more innovative, as measured by patent counts.\n\nbeta_hat &lt;- poisson_fit$par \n\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\n\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)\n\ndiff &lt;- y_pred_1 - y_pred_0\n\nmean_diff &lt;- mean(diff)\nmean_diff\n\n[1] 0.2178843"
  },
  {
    "objectID": "blog/Blog 2/index.html#airbnb-case-study",
    "href": "blog/Blog 2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nModeling Airbnb Reviews as a Proxy for Bookings\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression.\n\nairbnb &lt;- read_csv(\"/Users/daallen/Desktop/CLASSES/UCSD/MGTA495/quarto_website/blog/Blog 2/airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nairbnb_clean &lt;- airbnb %&gt;%\n  select(number_of_reviews, room_type, price, bathrooms, bedrooms,\n         review_scores_cleanliness, review_scores_location,\n         review_scores_value, instant_bookable) %&gt;%\n  drop_na()\n\nsummary(airbnb_clean)\n\n number_of_reviews  room_type             price           bathrooms    \n Min.   :  1.00    Length:30160       Min.   :   10.0   Min.   :0.000  \n 1st Qu.:  3.00    Class :character   1st Qu.:   70.0   1st Qu.:1.000  \n Median :  8.00    Mode  :character   Median :  103.0   Median :1.000  \n Mean   : 21.17                       Mean   :  140.2   Mean   :1.122  \n 3rd Qu.: 26.00                       3rd Qu.:  169.0   3rd Qu.:1.000  \n Max.   :421.00                       Max.   :10000.0   Max.   :6.000  \n    bedrooms      review_scores_cleanliness review_scores_location\n Min.   : 0.000   Min.   : 2.000            Min.   : 2.000        \n 1st Qu.: 1.000   1st Qu.: 9.000            1st Qu.: 9.000        \n Median : 1.000   Median :10.000            Median :10.000        \n Mean   : 1.151   Mean   : 9.202            Mean   : 9.415        \n 3rd Qu.: 1.000   3rd Qu.:10.000            3rd Qu.:10.000        \n Max.   :10.000   Max.   :10.000            Max.   :10.000        \n review_scores_value instant_bookable\n Min.   : 2.000      Mode :logical   \n 1st Qu.: 9.000      FALSE:24243     \n Median :10.000      TRUE :5917      \n Mean   : 9.334                      \n 3rd Qu.:10.000                      \n Max.   :10.000                      \n\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Review Counts\", x = \"Number of Reviews\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model\nWe now fit a Poisson regression where number_of_reviews is the outcome and the predictors include room type, price, and review scores.\n\nairbnb_clean &lt;- airbnb_clean %&gt;%\n  mutate(\n    room_type = as.factor(room_type),\n    instant_bookable = as.factor(instant_bookable)\n  )\n\nairbnb_model &lt;- glm(number_of_reviews ~ room_type + price + bathrooms + bedrooms +\n                      review_scores_cleanliness + review_scores_location +\n                      review_scores_value + instant_bookable,\n                    data = airbnb_clean,\n                    family = poisson(link = \"log\"))\n\nsummary(airbnb_model)\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + price + bathrooms + \n    bedrooms + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookableTRUE       3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nObservations:\n\nThe Poisson regression model provides insight into what listing characteristics are associated with the number of Airbnb reviews, used here as a proxy for bookings:\n\nRoom Type: Listings labeled as “Private room” and especially “Shared room” receive significantly fewer reviews than “Entire home/apt.” Shared rooms are associated with a ~25% decrease in review counts, all else equal.\nPrice: The effect of price is small and only marginally significant. This may suggest that within a moderate price range, price alone doesn’t strongly affect review count.\nBathrooms: More bathrooms are associated with significantly fewer reviews, possibly indicating larger properties with niche appeal.\nBedrooms: Listings with more bedrooms receive more reviews, reflecting higher demand for group or family accommodations.\nReview Scores:\n\nCleanliness positively impacts reviews — a one-point increase is associated with a ~11% increase in review count.\nSurprisingly, location and value scores have negative coefficients, possibly due to multicollinearity or guests leaving fewer reviews when expectations are already high.\n\nInstant Bookable: Being instantly bookable is associated with a large positive effect — listings with this feature receive ~33% more reviews, suggesting ease of booking drives demand.\n\nOverall, convenience (instant booking), cleanliness, and space (bedrooms) are key drivers of bookings, while room type has substantial impact on demand."
  }
]