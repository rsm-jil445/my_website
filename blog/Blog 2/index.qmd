---
title: "Poisson Regression Examples"
author: "Allen Li"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data
```{r, include=FALSE, message=FALSE, warning=FALSE}
install.packages("tidyverse")
```
```{r, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(ggplot2)

```

### Comparing Patents by Customer Status

```{r, include=FALSE, message=FALSE, warning=FALSE}
blueprinty <- read_csv("/Users/daallen/Desktop/CLASSES/UCSD/MGTA495/quarto_website/blog/Blog 2/blueprinty.csv")
```
```{r}
blueprinty %>%
  group_by(iscustomer) %>%
  summarise(
    mean_patents = mean(patents, na.rm = TRUE),
    median_patents = median(patents, na.rm = TRUE),
    sd_patents = sd(patents, na.rm = TRUE),
    n = n()
  )
```

```{r}

ggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +
  geom_histogram(binwidth = 1, position = "dodge", color = "white") +
  scale_fill_manual(values = c("#66c2a5", "#fc8d62"),
                    name = "Is Customer",
                    labels = c("No", "Yes")) +
  labs(
    title = "Patent Counts by Customer Status",
    x = "Number of Patents",
    y = "Count"
  ) +
  theme_minimal()
```

**Observations:**

> Based on the plot and summary table, existing customers (`iscustomer = 1`) have a higher mean number of patents (4.13) compared to non-customers (`iscustomer = 0`), who average 3.47 patents. The median and standard deviation follow a similar pattern. This suggests that Blueprinty's current customers tend to be more innovative or established, holding more patents on average than non-customers.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

### Comparing Regions and Ages by Customer Status

```{r}
blueprinty %>%
  group_by(iscustomer) %>%
  summarise(
    mean_age = mean(age, na.rm = TRUE),
    median_age = median(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    n = n()
  )
```

```{r}
ggplot(blueprinty, aes(x = region, fill = as.factor(iscustomer))) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("#a6cee3", "#1f78b4"),
                    name = "Is Customer",
                    labels = c("No", "Yes")) +
  labs(
    title = "Customer Status by Region",
    x = "Region",
    y = "Count"
  ) +
  theme_minimal()
```

**Observations:**

> Customers (`iscustomer = 1`) tend to be slightly older than non-customers. The mean age of customers is **26.9** years compared to **26.1** years for non-customers, with a similar spread in age.

> The regional distribution of customer status is not uniform. Notably, the **Northeast** region has a high concentration of customers, while regions like the **Northwest**, **South**, and **Southwest** have relatively fewer. This suggests that Blueprinty has stronger customer presence or market penetration in the Northeast.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

We assume that the number of patents awarded to each firm follows a Poisson distribution:

$$
Y_i \sim \text{Poisson}(\lambda_i), \quad \lambda_i = e^{\beta_0 + \beta_1 \cdot \text{iscustomer}_i}
$$

The probability mass function of the Poisson distribution is:

$$
f(Y_i \mid \lambda_i) = \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}
$$

The log-likelihood for \( n \) observations is:

$$
\log \mathcal{L}(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ -\lambda_i + Y_i \log(\lambda_i) - \log(Y_i!) \right]
= \sum_{i=1}^n \left[ -e^{\beta_0 + \beta_1 x_i} + Y_i(\beta_0 + \beta_1 x_i) - \log(Y_i!) \right]
$$

This is the function we will maximize using `optim()` to estimate the parameters \(\beta_0\) and \(\beta_1\).

```{r}
poisson_loglikelihood <- function(lambda, Y) {
  loglik <- sum(dpois(Y, lambda, log = TRUE))
  return(loglik)
}

Y_example <- blueprinty$patents
lambda_example <- mean(Y_example)
poisson_loglikelihood(lambda_example, Y_example)
```


```{r}
lambda_vals <- seq(0.1, 10, by = 0.1)

loglik_vals <- sapply(lambda_vals, function(l) poisson_loglikelihood(l, blueprinty$patents))

plot(lambda_vals, loglik_vals,
     type = "l",
     lwd = 2,
     col = "steelblue",
     main = "Log-Likelihood of Poisson Model vs. Lambda",
     xlab = expression(lambda),
     ylab = "Log-Likelihood")
```


### Analytical Derivation of the MLE for λ

If we assume that the observations \( Y_1, Y_2, \ldots, Y_n \) are independent and identically distributed from a Poisson distribution with parameter \( \lambda \), then the log-likelihood function is:

$$
\log \mathcal{L}(\lambda) = \sum_{i=1}^{n} \left[ -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right]
$$

Taking the derivative with respect to \( \lambda \) and setting it equal to zero:

$$
\frac{d}{d\lambda} \log \mathcal{L}(\lambda) = \sum_{i=1}^{n} \left[ -1 + \frac{Y_i}{\lambda} \right] = 0
$$

Solving:

$$
-n + \frac{\sum_{i=1}^{n} Y_i}{\lambda} = 0 \quad \Rightarrow \quad \lambda = \frac{1}{n} \sum_{i=1}^{n} Y_i = \bar{Y}
$$

Thus, the **maximum likelihood estimator** for \( \lambda \) is the sample mean:

$$
\hat{\lambda}_{\text{MLE}} = \bar{Y}
$$

This result aligns with our intuition — the Poisson distribution has mean \( \lambda \), so using the sample mean to estimate it makes sense.


```{r}
neg_loglik <- function(lambda, Y) {
  if (lambda <= 0) return(Inf)
  return(-poisson_loglikelihood(lambda, Y))
}

mle_result <- optim(par = 1,
                    fn = neg_loglik,
                    Y = blueprinty$patents,
                    method = "Brent",
                    lower = 0.001, upper = 20)

mle_result$par
```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{r}
poisson_regression_likelihood <- function(beta, Y, X) {
  lambda <- exp(X %*% beta) 
  loglik <- sum(dpois(Y, lambda, log = TRUE))  
  return(-loglik)  
}

X <- model.matrix(~ iscustomer + age + I(age^2) + region, data = blueprinty)

Y <- blueprinty$patents
```

```{r}
X <- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)

Y <- blueprinty$patents

init_beta <- rep(0, ncol(X))

poisson_fit <- optim(par = init_beta,
                     fn = poisson_regression_likelihood,
                     Y = Y, X = X,
                     method = "BFGS",
                     hessian = TRUE)

beta_hat <- poisson_fit$par

vcov_matrix <- solve(poisson_fit$hessian)

se_beta <- sqrt(diag(vcov_matrix))

coef_table <- tibble(
  Term = colnames(X),
  Estimate = beta_hat,
  Std_Error = se_beta
)

coef_table
```


```{r}
glm_fit <- glm(patents ~ age + I(age^2) + region + iscustomer,
               data = blueprinty,
               family = poisson(link = "log"))

summary(glm_fit)$coefficients
```

```{r}

glm_results <- summary(glm_fit)$coefficients
custom_results <- coef_table

comparison <- custom_results %>%
  mutate(glm_estimate = glm_results[, "Estimate"],
         glm_se = glm_results[, "Std. Error"])

comparison
```


**Interpretation of Results:**

The Poisson regression results show that:

- **Age** has a **positive** and significant effect on patent counts: older firms are more likely to have more patents, though the negative coefficient on **age squared** suggests a diminishing return — patent activity increases with age, but at a decreasing rate.
  
- The **iscustomer** coefficient is positive (0.061 in custom MLE; 0.208 in `glm()`), indicating that being a current customer of Blueprinty is associated with a higher expected number of patents. This supports the earlier exploratory findings that customers tend to be more patent-active.

- Regional effects are generally small and vary in sign. Compared to the baseline region (likely the one omitted by `model.matrix()`), regions like the **Northeast** and **Southwest** are slightly negatively associated with patent activity.

- The custom MLE estimates are **directionally consistent** with those from `glm()`, though they differ slightly in magnitude — likely due to convergence precision or default settings. Importantly, standard errors are also very similar, supporting the validity of the manual MLE approach.

Overall, the results make sense both statistically and intuitively. More established firms (by age and customer status) appear to be more innovative, as measured by patent counts.

```{r}
beta_hat <- poisson_fit$par 

X_0 <- X
X_0[, "iscustomer"] <- 0

X_1 <- X
X_1[, "iscustomer"] <- 1

y_pred_0 <- exp(X_0 %*% beta_hat)
y_pred_1 <- exp(X_1 %*% beta_hat)

diff <- y_pred_1 - y_pred_0

mean_diff <- mean(diff)
mean_diff
```


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


### Modeling Airbnb Reviews as a Proxy for Bookings

We model the number of reviews (as a proxy for bookings) using a Poisson regression.
```{r, include=FALSE, message=FALSE, warning=FALSE}
airbnb <- read_csv("/Users/daallen/Desktop/CLASSES/UCSD/MGTA495/quarto_website/blog/Blog 2/airbnb.csv")
```

```{r}
airbnb_clean <- airbnb %>%
  select(number_of_reviews, room_type, price, bathrooms, bedrooms,
         review_scores_cleanliness, review_scores_location,
         review_scores_value, instant_bookable) %>%
  drop_na()

summary(airbnb_clean)

ggplot(airbnb_clean, aes(x = number_of_reviews)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Review Counts", x = "Number of Reviews", y = "Count")
```

### Poisson Regression Model

We now fit a Poisson regression where `number_of_reviews` is the outcome and the predictors include room type, price, and review scores.

```{r}

airbnb_clean <- airbnb_clean %>%
  mutate(
    room_type = as.factor(room_type),
    instant_bookable = as.factor(instant_bookable)
  )

airbnb_model <- glm(number_of_reviews ~ room_type + price + bathrooms + bedrooms +
                      review_scores_cleanliness + review_scores_location +
                      review_scores_value + instant_bookable,
                    data = airbnb_clean,
                    family = poisson(link = "log"))

summary(airbnb_model)
```

**Observations:**

> The Poisson regression model provides insight into what listing characteristics are associated with the number of Airbnb reviews, used here as a proxy for bookings:
>
> - **Room Type**: Listings labeled as "Private room" and especially "Shared room" receive significantly fewer reviews than "Entire home/apt." Shared rooms are associated with a ~25% decrease in review counts, all else equal.
>
> - **Price**: The effect of price is small and only marginally significant. This may suggest that within a moderate price range, price alone doesn't strongly affect review count.
>
> - **Bathrooms**: More bathrooms are associated with significantly fewer reviews, possibly indicating larger properties with niche appeal.
>
> - **Bedrooms**: Listings with more bedrooms receive more reviews, reflecting higher demand for group or family accommodations.
>
> - **Review Scores**:
>   - **Cleanliness** positively impacts reviews — a one-point increase is associated with a ~11% increase in review count.
>   - Surprisingly, **location** and **value** scores have **negative coefficients**, possibly due to multicollinearity or guests leaving fewer reviews when expectations are already high.
>
> - **Instant Bookable**: Being instantly bookable is associated with a large positive effect — listings with this feature receive ~33% more reviews, suggesting ease of booking drives demand.
>
> Overall, convenience (instant booking), cleanliness, and space (bedrooms) are key drivers of bookings, while room type has substantial impact on demand.





